# import signal
import math
import scipy.io as sio
from scipy.signal import convolve2d as conv2
from scipy import signal
from scipy.signal.windows import triang
from scipy.signal import convolve2d as conv2
import pywt
import numpy as np
import segyio
import torch
from matplotlib import pyplot as plt

def get_dx_from_segy(seg_file_path):
    """
    ä» SEGY æ–‡ä»¶ä¸­æå–æ£€æ³¢ç‚¹ï¼ˆGroupXï¼‰çš„ç©ºé—´é‡‡æ ·é—´éš” dxï¼ˆå•ä½ï¼šç±³ï¼‰

    å‚æ•°:
        seg_file_path (str): SEGY æ–‡ä»¶è·¯å¾„

    è¿”å›:
        dx (float): ç©ºé—´é‡‡æ ·é—´éš”ï¼ˆå•ä½ï¼šç±³ï¼‰
    """
    with segyio.open(seg_file_path, "r", ignore_geometry=True) as f:
        group_x = f.attributes(segyio.TraceField.GroupX)[:]  # è·å–æ‰€æœ‰æ£€æ³¢ç‚¹Xåæ ‡
        print(group_x[:10])  # æ‰“å°å‰ 10 ä¸ªæ¥æ”¶ç‚¹ä½ç½®
        dxs = np.diff(group_x)  # ç›¸é‚»æ£€æ³¢ç‚¹ä¹‹é—´çš„é—´è·
        dx = np.median(dxs)     # ä½¿ç”¨ä¸­ä½æ•°é¿å…å¼‚å¸¸å€¼å½±å“
        print(f"ç©ºé—´é‡‡æ ·é—´éš” dx = {dx} ç±³")
    return dx
def fk_spectra(data, dt, dx, L=6):
    """
    f-k(é¢‘ç‡-æ³¢æ•°)é¢‘è°±åˆ†æ
    :param data: äºŒç»´çš„åœ°éœ‡æ•°æ®
    :param dt: æ—¶é—´é‡‡æ ·é—´éš”
    :param dx: é“é—´è·
    :param L: å¹³æ»‘çª—å£
    :return: S(é¢‘è°±ç»“æœ), f(é¢‘ç‡èŒƒå›´), k(æ³¢æ•°èŒƒå›´)
    """
    print(data.shape)
    data = np.array(data)
    [nt, nx] = data.shape  # è·å–æ•°æ®ç»´åº¦
    # è®¡ç®—nkå’Œnfæ˜¯ä¸ºäº†åŠ å¿«å‚…é‡Œå¶å˜æ¢é€Ÿåº¦,ç­‰åŒäºnextpow2
    i = 0
    while (2 ** i) <= nx:
        i = i + 1
    nk = 4 * 2 ** i
    j = 0
    while (2 ** j) <= nt:
        j = j + 1
    nf = 4 * 2 ** j
    S = np.fft.fftshift(abs(np.fft.fft2(data, (nf, nk))))  # äºŒç»´å‚…é‡Œå¶å˜æ¢
    H1 = np.hamming(L)
    # è®¾ç½®æ±‰æ˜çª—å£å¤§å°ï¼Œæ±‰æ˜çª—çš„æ—¶åŸŸæ³¢å½¢ä¸¤ç«¯ä¸èƒ½åˆ°é›¶ï¼Œè€Œæµ·å®çª—æ—¶åŸŸä¿¡å·ä¸¤ç«¯æ˜¯é›¶ã€‚ä»é¢‘åŸŸå“åº”æ¥çœ‹ï¼Œæ±‰æ˜çª—èƒ½å¤Ÿå‡å°‘å¾ˆè¿‘çš„æ—ç“£æ³„éœ²
    H = (H1.reshape(L, -1)) * (H1.reshape(1, L))
    S = signal.convolve2d(S, H, boundary='symm', mode='same')  # æ±‰æ˜å¹³æ»‘
    S = S[nf // 2:nf, :]
    f = np.arange(0, nf / 2, 1)
    f = f / nf / dt
    k = np.arange(-nk / 2, nk / 2, 1)
    k = k / nk / dx
    return S, k, f
# def fk_spectra_1(data, dt, dx, L=6):
#     """
#     f-k(é¢‘ç‡-æ³¢æ•°)é¢‘è°±åˆ†æ
#     :param data: äºŒç»´çš„åœ°éœ‡æ•°æ®
#     :param dt: æ—¶é—´é‡‡æ ·é—´éš”
#     :param dx: é“é—´è·
#     :param L: å¹³æ»‘çª—å£
#     :return: S(é¢‘è°±ç»“æœ), f(é¢‘ç‡èŒƒå›´), k(æ³¢æ•°èŒƒå›´)
#     """
#     data = np.array(data)
#     [nt, nx] = data.shape  # è·å–æ•°æ®ç»´åº¦
#     # è®¡ç®—nkå’Œnfæ˜¯ä¸ºäº†åŠ å¿«å‚…é‡Œå¶å˜æ¢é€Ÿåº¦,ç­‰åŒäºnextpow2
#     i = 0
#     while (2 ** i) <= nx:
#         i = i + 1
#     nk = 4 * 2 ** i
#     j = 0
#     while (2 ** j) <= nt:
#         j = j + 1
#     nf = 4 * 2 ** j
#     S = np.fft.fftshift(abs(np.fft.fft2(data, (nf, nk))))  # äºŒç»´å‚…é‡Œå¶å˜æ¢
#     H1 = np.hamming(L)
#     # è®¾ç½®æ±‰æ˜çª—å£å¤§å°ï¼Œæ±‰æ˜çª—çš„æ—¶åŸŸæ³¢å½¢ä¸¤ç«¯ä¸èƒ½åˆ°é›¶ï¼Œè€Œæµ·å®çª—æ—¶åŸŸä¿¡å·ä¸¤ç«¯æ˜¯é›¶ã€‚ä»é¢‘åŸŸå“åº”æ¥çœ‹ï¼Œæ±‰æ˜çª—èƒ½å¤Ÿå‡å°‘å¾ˆè¿‘çš„æ—ç“£æ³„éœ²
#     H = (H1.reshape(L, -1)) * (H1.reshape(1, L))
#     S = signal.convolve2d(S, H, boundary='symm', mode='same')  # æ±‰æ˜å¹³æ»‘
#     S = S[nf // 2:nf, :]
#     # f = np.arange(0, nf / 2, 1)
#     # f = f / nf / dt
#     f = np.fft.fftfreq(nf, dt)
#     f = f[:nf // 2]  # åªä¿ç•™æ­£é¢‘ç‡
#     # k = np.arange(-nk / 2, nk / 2, 1)
#     # k = k / nk / dx
#     k = np.fft.fftfreq(nk, dx)
#     k = np.fft.fftshift(k)  # å› ä¸º S ä¹Ÿåšäº† fftshift
#     return S, k, f
def plot_seismic_f_k_npy(seismic_data, save=False, save_path=None, show=False):
    dx = 125
    dt = 0.008
    S, k, f = fk_spectra(seismic_data, dt, dx)
    S[S <= 0] = 1e-10  # é¿å…å¯¹æ•°åŸŸä¸­çš„é›¶å€¼æˆ–è´Ÿå€¼
    amplitude_db = 10 * np.log10(S)


    plt.figure(figsize=(6, 6))
    plt.pcolormesh(k, f, amplitude_db, shading='auto', cmap='viridis', vmin=0, vmax=100)
    # plt.colorbar(label='Amplitude (dB)')
    plt.colorbar()
    plt.xlabel('k [c/m]')
    plt.ylabel('f [Hz]')
    # å€’è½¬ y è½´ï¼Œä½¿å¾—ä½é¢‘åœ¨åº•éƒ¨ï¼Œé«˜é¢‘åœ¨é¡¶éƒ¨
    plt.gca().invert_yaxis()
    x_ticks = np.linspace(k.min(), k.max(), 3)  # ç”Ÿæˆ 5 ä¸ªç­‰é—´è·çš„åˆ»åº¦
    plt.xticks(x_ticks)  # è®¾ç½® x è½´åˆ»åº¦
    # plt.title('f-k Spectrum')
    plt.subplots_adjust(left=0.18, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)
    # plt.show()
    # ä½ å¯ä»¥é€šè¿‡è°ƒæ•´ k å’Œ f çš„èŒƒå›´æ¥æ”¾å¤§å›¾åƒ
    # k_min, k_max = -0.01025, 0  # è®¾ç½®ä½ å¸Œæœ›æ˜¾ç¤ºçš„æ³¢æ•°èŒƒå›´
    # f_min, f_max = 0, 55  # è®¾ç½®ä½ å¸Œæœ›æ˜¾ç¤ºçš„é¢‘ç‡èŒƒå›´

    # ç»˜åˆ¶é¢‘è°±å›¾
    # plt.figure(figsize=(6, 6))
    # plt.pcolormesh(k, f, amplitude_db, shading='auto', cmap='viridis', vmin=0, vmax=100)
    #
    # # æ”¾å¤§æ˜¾ç¤ºæ„Ÿå…´è¶£çš„åŒºåŸŸ
    # plt.xlim(k_min, k_max)  # é™åˆ¶ x è½´ï¼ˆæ³¢æ•° kï¼‰çš„èŒƒå›´
    # plt.ylim(f_min, f_max)  # é™åˆ¶ y è½´ï¼ˆé¢‘ç‡ fï¼‰çš„èŒƒå›´
    # plt.colorbar()
    # plt.xlabel('k [c/m]')
    # plt.ylabel('f [Hz]')
    # # è®¾ç½® x è½´åˆ»åº¦
    # x_ticks = np.linspace(k_min, k_max, 3)  # ç”Ÿæˆ 3 ä¸ªç­‰é—´è·çš„åˆ»åº¦
    # plt.xticks(x_ticks)  # è®¾ç½® x è½´åˆ»åº¦
    # plt.subplots_adjust(left=0.18, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)

    if save and save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“· Saved figure: {save_path}")
    elif show:
        plt.show()
    plt.close()
def plot_seismic_tensor(seis_tensor,extent_time, save=False, save_path=None, show=True):
    plt.figure(figsize=(4.5, 6))
    plt.imshow(seis_tensor, cmap='gray', extent=extent_time, aspect='auto', vmin=-1, vmax=1)
    # plt.colorbar(label='')
    plt.xlabel('Trace')
    plt.ylabel('Time(ms)')
    plt.title('')
    # è°ƒæ•´è¾¹è·å’Œé—´è·
    plt.subplots_adjust(left=0.18, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)
    if save and save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“· Saved figure: {save_path}")
    elif show:
        plt.show()
    plt.close()
def plot_seismic_npy(path_file,extent_time,save=False,save_path=None, show=False):
    dataset_t = path_file
    seis_tensor = torch.tensor(dataset_t)
    plot_seismic_tensor(seis_tensor, extent_time, save=save, save_path=save_path, show=show)
def calculate_snr(target_v, output_v):
    # å¦‚æœæ˜¯Tensorï¼Œè½¬ä¸ºnumpy
    if isinstance(output_v, torch.Tensor):
        output_v = output_v.detach().cpu().numpy()
    if isinstance(target_v, torch.Tensor):
        target_v = target_v.detach().cpu().numpy()

    # flattenåæ•´ä½“è®¡ç®—èƒ½é‡
    origSignal = target_v.flatten()
    errorSignal = (target_v - output_v).flatten()

    signal_power = np.sum(origSignal ** 2)
    noise_power = np.sum(errorSignal ** 2)

    # é¿å…é™¤é›¶é”™è¯¯
    if noise_power == 0:
        return float('inf')

    snr = 10 * math.log10(signal_power / noise_power)
    return snr
def calculate_rmse(origin, predicted):
    # åŠ è½½æ•°æ®
    print(origin.shape)
    print(predicted.shape)

    # ç¡®ä¿æ•°æ®å½¢çŠ¶ä¸€è‡´
    if origin.shape != predicted.shape:
        raise ValueError("Origin and predicted signals must have the same shape.")

    # è®¡ç®—å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰
    mse = np.mean((predicted - origin) ** 2)
    rmse = np.sqrt(mse)

    return rmse
def get_info_seg(seg_file_path):
    with segyio.open(seg_file_path, 'r', ignore_geometry=True) as f:
        # è¯»å–æ‰€æœ‰åœ°éœ‡é“æ•°æ®
        f.mmap()
        sourceX = f.attributes(segyio.TraceField.SourceX)[:]
        nTrace = f.tracecount
        nSample = f.bin[segyio.BinField.Samples]
        startT = 0
        deltaT = f.bin[segyio.BinField.Interval]
        print("     Number of Trace   = %d" % (nTrace))
        print("     Number of Samples = %d" % (nSample))
        print("     Start Samples     = %d" % (startT))
        print("     Sampling Rate     = %d" % (deltaT))
        data = np.asarray([np.copy(trace) for trace in f.trace])
    data = data.T
    time_length = (nSample*deltaT)/1000.0
    extent_time = [0, nTrace, time_length, 0]
    return data, nSample,extent_time
def get_mat(mat_file_path):
    dataset_p = sio.loadmat(mat_file_path)
    print(dataset_p.keys())
    keys = list(dataset_p.keys())
    last_key = keys[-1]
    dataset_p = dataset_p[last_key]  # å‡è®¾æ–‡ä»¶ä¸­å­˜æœ‰å­—ç¬¦å˜é‡æ˜¯matrixï¼Œ
    return dataset_p

def predict_data_extract_paired_patches(noise_data, clean_data, patch_length=256, stride=128):
    """
    ä»å™ªå£°å’Œå¹²å‡€æ•°æ®ä¸­æ»‘çª—æå–ä¸€ä¸€å¯¹åº”çš„ patchã€‚
    è¾“å…¥ï¼š
        noise_data: shape = (1151, 16000)
        clean_data: shape = (1151, 16000)
    è¾“å‡ºï¼š
        noise_patches: shape = (N, 1, patch_length)
        clean_patches: shape = (N, 1, patch_length)
    """
    n_samples, n_traces = noise_data.shape
    noise_patches = []
    clean_patches = []

    # std_thresh = 0
    for trace in range(n_traces):
        start = 0
        while start + patch_length <= n_samples:
            n_patch = noise_data[start:start+patch_length, trace]
            c_patch = clean_data[start:start+patch_length, trace]


            noise_patches.append(n_patch[np.newaxis, :])
            clean_patches.append(c_patch[np.newaxis, :])
            start += stride

        # è‹¥æœ€åä¸€æ®µä¸å¤Ÿ patch_lengthï¼Œåˆ™ä»å°¾éƒ¨å¾€å‰æˆªå–å®Œæ•´ patch
        if start < n_samples:
            # end = n_samples
            # start_last = max(end - patch_length, 0)
            #
            # n_patch = noise_data[start_last:end, trace]
            # c_patch = clean_data[start_last:end, trace]
            #
            # noise_patches.append(n_patch[np.newaxis, :])
            # clean_patches.append(c_patch[np.newaxis, :])

            n_remain = noise_data[start:, trace]
            c_remain = clean_data[start:, trace]
            pad_len = patch_length - len(n_remain)

            n_padded = np.pad(n_remain, (0, pad_len), mode='constant')
            c_padded = np.pad(c_remain, (0, pad_len), mode='constant')

            noise_patches.append(n_padded[np.newaxis, :])
            clean_patches.append(c_padded[np.newaxis, :])

    return (
        np.array(noise_patches),  # shape: (N, 1, patch_length)
        np.array(clean_patches)
    )
def extract_paired_patches(noise_data, clean_data, patch_length=256, stride=128):
    """
    ä»å™ªå£°å’Œå¹²å‡€æ•°æ®ä¸­æ»‘çª—æå–ä¸€ä¸€å¯¹åº”çš„ patchã€‚
    è¾“å…¥ï¼š
        noise_data: shape = (1151, 16000)
        clean_data: shape = (1151, 16000)
    è¾“å‡ºï¼š
        noise_patches: shape = (N, 1, patch_length)
        clean_patches: shape = (N, 1, patch_length)
    """
    n_samples, n_traces = noise_data.shape
    noise_patches = []
    clean_patches = []
    std_thresh = 1e-3
    # std_thresh = 0
    for trace in range(n_traces):
        start = 0
        while start + patch_length <= n_samples:
            n_patch = noise_data[start:start+patch_length, trace]
            c_patch = clean_data[start:start+patch_length, trace]

            # æ·»åŠ ç­›é€‰æ¡ä»¶
            if np.sum(c_patch) != 0 and np.std(c_patch) > std_thresh:
                noise_patches.append(n_patch[np.newaxis, :])
                clean_patches.append(c_patch[np.newaxis, :])
            start += stride

        # è¡¥è¶³æœ€åä¸€æ®µ
        if start < n_samples:
            # end = n_samples
            # start_last = max(end - patch_length, 0)
            #
            # n_patch = noise_data[start_last:end, trace]
            # c_patch = clean_data[start_last:end, trace]

            n_remain = noise_data[start:, trace]
            c_remain = clean_data[start:, trace]
            pad_len = patch_length - len(n_remain)

            n_patch = np.pad(n_remain, (0, pad_len), mode='constant')
            c_patch = np.pad(c_remain, (0, pad_len), mode='constant')

            if np.sum(c_patch) != 0 and np.std(n_patch) > std_thresh:
                noise_patches.append(n_patch[np.newaxis, :])
                clean_patches.append(c_patch[np.newaxis, :])

    return (
        np.array(noise_patches),  # shape: (N, 1, patch_length)
        np.array(clean_patches)
    )

def extract_paired_patches_3d(noise_data, clean_data, patch_length=256, stride=128):
    """
    æ”¯æŒé€šé“æ•°çš„ç‰ˆæœ¬ã€‚
    è¾“å…¥ï¼š
        noise_data: shape = (1151, 16000, 2)
        clean_data: shape = (1151, 16000, 1)
    è¾“å‡ºï¼š
        noise_patches: shape = (N, 2, patch_length)
        clean_patches: shape = (N, 1, patch_length)
    """
    n_samples, n_traces, n_channels_noise = noise_data.shape
    _, _, n_channels_clean = clean_data.shape

    noise_patches = []
    clean_patches = []
    std_thresh = 1e-3

    for trace in range(n_traces):
        start = 0
        while start + patch_length <= n_samples:
            n_patch = noise_data[start:start+patch_length, trace, :]  # (patch_len, 2)
            c_patch = clean_data[start:start+patch_length, trace, :]  # (patch_len, 1)

            n_patch = n_patch.T  # -> (2, patch_len)
            c_patch = c_patch.T  # -> (1, patch_len)

            if np.sum(c_patch) != 0 and np.std(n_patch) > std_thresh:
                noise_patches.append(n_patch)
                clean_patches.append(c_patch)

            start += stride

        # è¡¥æœ€åä¸€æ®µ
        if start < n_samples:
            # n_remain = noise_data[start:, trace, :]  # (æ®‹é•¿, 2)
            # c_remain = clean_data[start:, trace, :]  # (æ®‹é•¿, 1)
            # pad_len = patch_length - len(n_remain)
            #
            # n_patch = np.pad(n_remain, ((0, pad_len), (0, 0)), mode='constant').T  # -> (2, patch_len)
            # c_patch = np.pad(c_remain, ((0, pad_len), (0, 0)), mode='constant').T  # -> (1, patch_len)
            start_last = n_samples - patch_length
            n_patch = noise_data[start_last:start_last + patch_length, trace, :].T
            c_patch = clean_data[start_last:start_last + patch_length, trace, :].T

            if np.sum(c_patch) != 0 and np.std(n_patch) > std_thresh:
                noise_patches.append(n_patch)
                clean_patches.append(c_patch)

    return (
        np.array(noise_patches),  # shape: (N, 2, patch_length)
        np.array(clean_patches)   # shape: (N, 1, patch_length)
    )


def extract_data_2d_extract_paired_patches(noise_data, clean_data, patch_length=64, stride=32):
    """
    ä»å™ªå£°å’Œå¹²å‡€æ•°æ®ä¸­æå– 2D patchï¼ˆæ—¶é—´ Ã— éœ‡é“ï¼‰ã€‚
    è¾“å‡ºï¼š
        noise_patches: shape = (N, 1, patch_length, patch_length)
        clean_patches: shape = (N, 1, patch_length, patch_length)
    """
    n_samples, n_traces = noise_data.shape
    noise_patches = []
    clean_patches = []

    std_thresh = 1e-3

    half_patch = patch_length // 2

    for center_trace in range(half_patch, n_traces - half_patch):
        start = 0
        while start + patch_length <= n_samples:
            # æ—¶é—´çª—å£
            time_slice = slice(start, start + patch_length)
            # ç©ºé—´çª—å£ï¼ˆéœ‡é“ï¼‰
            trace_slice = slice(center_trace - half_patch, center_trace + half_patch)



            n_patch = noise_data[time_slice, trace_slice]    # shape: (patch_length, patch_length)
            c_patch = clean_data[time_slice, trace_slice]

            # ç­›é€‰æ¡ä»¶ï¼šé¿å…å…¨é›¶å’Œæ ‡å‡†å·®è¿‡å°çš„patch
            if np.sum(c_patch) != 0 and np.std(c_patch) > std_thresh:
                noise_patches.append(n_patch[np.newaxis, :, :])  # (1, patch_length, patch_length)
                clean_patches.append(c_patch[np.newaxis, :, :])
            # noise_patches.append(n_patch[np.newaxis, :, :])  # shape: (1, patch_length, patch_length)
            # clean_patches.append(c_patch[np.newaxis, :, :])
            start += stride

        # å¤„ç†æœ€åä¸€æ®µï¼ˆpaddingï¼‰
        if start < n_samples:
            n_remain = noise_data[start:, center_trace - half_patch:center_trace + half_patch]
            c_remain = clean_data[start:, center_trace - half_patch:center_trace + half_patch]

            pad_len = patch_length - n_remain.shape[0]
            n_padded = np.pad(n_remain, ((0, pad_len), (0, 0)), mode='constant')
            c_padded = np.pad(c_remain, ((0, pad_len), (0, 0)), mode='constant')

            if np.sum(c_patch) != 0 and np.std(c_patch) > std_thresh:
                noise_patches.append(n_patch[np.newaxis, :, :])
                clean_patches.append(c_patch[np.newaxis, :, :])
            # noise_patches.append(n_padded[np.newaxis, :, :])
            # clean_patches.append(c_padded[np.newaxis, :, :])

    noise_patches = np.array(noise_patches)  # shape: (N, 1, patch_length, patch_length)
    clean_patches = np.array(clean_patches)
    return noise_patches, clean_patches



def predict_data_extract_paired_patches_3d(noise_data, clean_data, patch_length=256, stride=128):
    """
    æ”¯æŒé€šé“æ•°çš„ç‰ˆæœ¬ã€‚
    è¾“å…¥ï¼š
        noise_data: shape = (1151, 16000, 2)
        clean_data: shape = (1151, 16000, 1)
    è¾“å‡ºï¼š
        noise_patches: shape = (N, 2, patch_length)
        clean_patches: shape = (N, 1, patch_length)
    """
    n_samples, n_traces, n_channels_noise = noise_data.shape
    # print(clean_data.shape)
    _, _, n_channels_clean = clean_data.shape

    noise_patches = []
    clean_patches = []
    std_thresh = 1e-3

    for trace in range(n_traces):
        start = 0
        while start + patch_length <= n_samples:
            n_patch = noise_data[start:start+patch_length, trace, :]  # (patch_len, 2)
            c_patch = clean_data[start:start+patch_length, trace, :]  # (patch_len, 1)

            n_patch = n_patch.T  # -> (2, patch_len)
            c_patch = c_patch.T  # -> (1, patch_len)

            noise_patches.append(n_patch)
            clean_patches.append(c_patch)

            # if np.sum(c_patch) != 0 and np.std(n_patch) > std_thresh:
            #     noise_patches.append(n_patch)
            #     clean_patches.append(c_patch)

            start += stride

        # è¡¥æœ€åä¸€æ®µ
        if start < n_samples:
            # n_remain = noise_data[start:, trace, :]  # (æ®‹é•¿, 2)
            # c_remain = clean_data[start:, trace, :]  # (æ®‹é•¿, 1)
            # pad_len = patch_length - len(n_remain)
            #
            # n_patch = np.pad(n_remain, ((0, pad_len), (0, 0)), mode='constant').T  # -> (2, patch_len)
            # c_patch = np.pad(c_remain, ((0, pad_len), (0, 0)), mode='constant').T  # -> (1, patch_len)
            start_last = n_samples - patch_length
            n_patch = noise_data[start_last:start_last + patch_length, trace, :].T
            c_patch = clean_data[start_last:start_last + patch_length, trace, :].T

            noise_patches.append(n_patch)
            clean_patches.append(c_patch)

            # if np.sum(c_patch) != 0 and np.std(n_patch) > std_thresh:
            #     noise_patches.append(n_patch)
            #     clean_patches.append(c_patch)

    return (
        np.array(noise_patches),  # shape: (N, 2, patch_length)
        np.array(clean_patches)   # shape: (N, 1, patch_length)
    )

def cwt_real_imag_concat_2d(data, fs=125, totalscal=32, wavelet='cmor1.5-1.0'):
    """
    å¯¹å½¢çŠ¶ä¸º (B, C, H, W) çš„ torch.Tensor æ•°æ®è¿›è¡Œ CWTï¼Œ
    å¯¹æœ€åä¸€ä¸ªç»´åº¦ï¼ˆå®½åº¦æ–¹å‘ï¼‰åšCWTï¼Œ
    å°†å®éƒ¨ä¸è™šéƒ¨å †å æˆ (B, C*2, H, W) è¿”å›ã€‚

    å‚æ•°ï¼š
        data: torch.Tensorï¼Œå½¢çŠ¶ä¸º (B, C, H, W)
        fs: é‡‡æ ·é¢‘ç‡
        totalscal: CWTå°ºåº¦æ•°é‡
        wavelet: å°æ³¢åç§°ï¼Œé»˜è®¤ 'cmor1.5-1.0'

    è¿”å›ï¼š
        torch.Tensorï¼Œå½¢çŠ¶ä¸º (B, C*2, H, W)
    """
    assert data.ndim == 4, "è¾“å…¥å¿…é¡»æ˜¯å½¢çŠ¶ (B, C, H, W)"
    B, C, S, T = data.shape

    Fc = pywt.central_frequency(wavelet)
    c = 2 * Fc * totalscal
    scales = c / np.arange(1, totalscal + 1)

    output = np.zeros((B, C * 2, S, T), dtype=np.float32)

    assert data.ndim == 4, "è¾“å…¥å¿…é¡»æ˜¯å½¢çŠ¶ä¸º (B, C, S, T)"
    B, C, S, T = data.shape

    # æ„é€ å°æ³¢å°ºåº¦
    Fc = pywt.central_frequency(wavelet)
    c = 2 * Fc * totalscal
    scales = c / np.arange(1, totalscal + 1)

    # åˆå§‹åŒ–è¾“å‡º (B, C*2, S, T)
    output = np.zeros((B, C * 2, S, T), dtype=np.float32)

    # éå†æ¯ä¸ªæ ·æœ¬ã€é€šé“ã€éœ‡é“
    for b in range(B):
        for c_idx in range(C):
            for t in range(T):
                sig = data[b, c_idx, :, t]  # shape: (S,) â€”â€” æ¯æ¡éœ‡é“çš„æ—¶é—´åºåˆ—
                if isinstance(sig, torch.Tensor):
                    sig = sig.cpu().numpy()

                # å°æ³¢å˜æ¢
                coefs, _ = pywt.cwt(sig, scales, wavelet, sampling_period=1 / fs)  # (scales, S)
                real_part = np.mean(np.real(coefs), axis=0)  # (S,)
                imag_part = np.mean(np.imag(coefs), axis=0)  # (S,)

                output[b, c_idx * 2, :, t] = real_part
                output[b, c_idx * 2 + 1, :, t] = imag_part

    return torch.from_numpy(output).float()


def cwt_real_imag_concat(data, fs=125):
    """
    å¯¹å½¢çŠ¶ä¸º (B, C, 256) çš„ torch.Tensor æ•°æ®è¿›è¡Œ CWTï¼Œå¹¶å°†å®éƒ¨ä¸è™šéƒ¨å †å æˆ (B, 2, 256)

    å‚æ•°ï¼š
        data: torch.Tensorï¼Œå½¢çŠ¶ä¸º (B, 1, 256)
        wavelet: å°æ³¢å‡½æ•°åç§°ï¼Œå¦‚ 'cmor1.5-1.0'
        totalscal: ä½¿ç”¨çš„å°ºåº¦æ•°é‡ï¼ˆå³é¢‘ç‡å±‚çº§æ•°ï¼‰
        fs: é‡‡æ ·é¢‘ç‡ï¼ˆHzï¼‰

    è¿”å›ï¼š
        torch.Tensorï¼Œå½¢çŠ¶ä¸º (B, 3, 256)
    """


    # wavelet = 'cmor3-3'
    assert data.ndim == 3, "è¾“å…¥å¿…é¡»æ˜¯å½¢çŠ¶ä¸º (B, 1, T) çš„ Tensor"
    totalscal = 32
    wavelet = 'cmor1.5-1.0'
    B, C, T = data.shape

    Fc = pywt.central_frequency(wavelet)
    c = 2 * Fc * totalscal
    scales = c / np.arange(1, totalscal + 1)

    # åˆå§‹åŒ–è¾“å‡ºå®éƒ¨å’Œè™šéƒ¨æ•°ç»„
    # real_out = np.zeros((B, T), dtype=np.float32)
    # imag_out = np.zeros((B, T), dtype=np.float32)
    # features = np.zeros((B, 1, T), dtype=np.float32)
    # åˆå§‹åŒ–è¾“å‡ºæ•°ç»„ (B, C*2, T)
    output = np.zeros((B, C * 2, T), dtype=np.float32)
    for b in range(B):
        for c_idx in range(C):
            sig = data[b, c_idx].cpu().numpy()  # shape: (T,)
            coefs, _ = pywt.cwt(sig, scales, wavelet, sampling_period=1 / fs)  # (scales, T)
            real_part = np.mean(np.real(coefs), axis=0)  # (T,)
            imag_part = np.mean(np.imag(coefs), axis=0)  # (T,)
            output[b, c_idx * 2] = real_part
            output[b, c_idx * 2 + 1] = imag_part
    # for b in range(B):
    #     sig = data[b, 0].cpu().numpy()  # shape: (256,)
    #     coefs, _ = pywt.cwt(sig, scales, wavelet, sampling_period=1/fs)  # (scales, T)
    #     real_out[b] = np.mean(np.real(coefs), axis=0)  # å¹³å‡æŠ•å½±åˆ°æ—¶é—´ç»´åº¦
    #     imag_out[b] = np.mean(np.imag(coefs), axis=0)
        # magnitude = np.abs(coefs)  # (scales, T)
        # mean_mag = np.mean(magnitude, axis=0)  # (T,)
        # features[b, 0] = mean_mag

    # print("--------------")
    # print(real_out.shape)
    # å †å æˆ (B, 2, T)
    # output = np.stack([real_out, imag_out], axis=1)
    # print(output.shape)
    return torch.from_numpy(output).float()
    # return torch.from_numpy(features).float()



def cwt_real_imag_concat_scales(data, fs=125):
    """
    å¯¹å½¢çŠ¶ä¸º (B, C, T) çš„ torch.Tensor æ•°æ®è¿›è¡Œ CWTï¼Œä¿ç•™æ¯ä¸ªå°ºåº¦çš„å®éƒ¨å’Œè™šéƒ¨ï¼Œ
    å¹¶å°†å…¶å †å ä¸º (B, 2*C, T, S) çš„è¾“å‡ºã€‚

    å‚æ•°ï¼š
        data: torch.Tensorï¼Œå½¢çŠ¶ä¸º (B, C, T)
        fs: é‡‡æ ·é¢‘ç‡ï¼ˆHzï¼‰

    è¿”å›ï¼š
        torch.Tensorï¼Œå½¢çŠ¶ä¸º (B, 2*C, T, S)
    """
    assert data.ndim == 3, "è¾“å…¥å¿…é¡»æ˜¯å½¢çŠ¶ä¸º (B, C, T) çš„ Tensor"
    totalscal = 64
    wavelet = 'cmor1.5-1.0'
    B, C, T = data.shape

    Fc = pywt.central_frequency(wavelet)
    c = 2 * Fc * totalscal
    scales = c / np.arange(1, totalscal + 1)

    # è¾“å‡º shape: (B, 2*C, T, S)
    output = np.zeros((B, 2 * C, T, totalscal), dtype=np.float32)

    for b in range(B):
        for c_idx in range(C):
            sig = data[b, c_idx].cpu().numpy()  # (T,)
            coefs, _ = pywt.cwt(sig, scales, wavelet, sampling_period=1 / fs)  # (S, T)

            real_part = np.real(coefs).T  # (T, S)
            imag_part = np.imag(coefs).T  # (T, S)

            output[b, c_idx * 2, :, :] = real_part
            output[b, c_idx * 2 + 1, :, :] = imag_part

    return torch.from_numpy(output).float()

# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # path = 'data/data_mat_npy_sgy/part1_4_npy_mat/2007BP_part1_11shot.sgy'
    #
    # noise_data_original = np.load('data/data_mat_npy_sgy/part1_4_npy_mat/snr_0.0382144709989505.npy')
    # noise_data = get_mat('data/data_mat_npy_sgy/VMD_noise/VMD_K/VMD_K_snr_0.0382144709989505.mat')
    #
    # origin, nSample, extent_time = get_info_seg(path)
    # plot_seismic_npy(noise_data, extent_time, show=True)
    # plot_seismic_f_k_npy(noise_data,show=True)
    # plot_seismic_f_k_npy(origin,show=True)
    # plot_seismic_npy((original_noise_data - reconstructed_data), extent_time, show=True)
    # plot_seismic_f_k_npy(noise_data,show=True)
    # plot_seismic_f_k_npy(origin,show=True)
    # sea_dx = 25,2007BP_dx = 125ï¼Œland_dx = 25ç±³
    vmd_noise = get_mat('data/data_mat_npy_sgy/VMD_noise/VMD_K.mat')
    clean_data, t, time = get_info_seg('data/2007BP_synthetic_train.sgy')
    clean_data = np.expand_dims(clean_data, axis=-1)
    # clean_data = np.expand_dims(clean_data, axis=-1)
    # print(vmd_noise.shape)
    # print(clean_data.shape)
    # n,c = extract_paired_patches_3d(noise_data=vmd_noise,clean_data=clean_data,patch_length=256,stride=128)
    # print(n.shape)
    # print(c.shape)
    np.random.seed(42)
    sigma = 700
    noise = np.random.normal(0, sigma / 255.0, clean_data.shape)
    # print(clean_data.shape)
    noise_data = clean_data + noise

    noise_patches, origin_patches = extract_paired_patches_3d(clean_data=clean_data, noise_data=vmd_noise,
                                                              patch_length=256, stride=128)

    print(noise_patches.shape)
    print(origin_patches.shape)




